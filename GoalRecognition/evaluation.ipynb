{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import import_ipynb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fetch_data import fewer_plot_names\n",
    "#from bias_inspection import detect_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(y_pred, y_true, ids):\n",
    "    data_dict = {}\n",
    "    for s_id in ids['ids']:\n",
    "        if s_id in data_dict:\n",
    "            continue\n",
    "        data_dict[s_id] = {}\n",
    "        for g_id in ids['goal_ids']:\n",
    "            if g_id.startswith(s_id):\n",
    "                if g_id in data_dict[s_id]:\n",
    "                    continue\n",
    "                data_dict[s_id][g_id] = {'y_pred': [], 'y_true': -1}\n",
    "                for y in range(len(y_pred)):\n",
    "                    if ids['goal_ids'][y] == g_id:\n",
    "                        data_dict[s_id][g_id]['y_pred'].append(y_pred[y])\n",
    "                        data_dict[s_id][g_id]['y_true'] = y_true[y]\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_scores(data):\n",
    "    mean_accuracy = 0\n",
    "    std_cp = 0\n",
    "    \n",
    "    for s_id in data:\n",
    "        accuracies = []\n",
    "        avg_cp = 0\n",
    "        for g_id in data[s_id]:\n",
    "            cp = -1\n",
    "            data[s_id][g_id]['accuracy'] = 0\n",
    "            for y_idx in range(len(data[s_id][g_id]['y_pred'])):\n",
    "                if np.argmax(data[s_id][g_id]['y_pred'][y_idx]) == np.argmax(data[s_id][g_id]['y_true']):\n",
    "                    data[s_id][g_id]['accuracy'] += 1\n",
    "                    cp = y_idx + 1\n",
    "                else:\n",
    "                    cp = -1\n",
    "            if cp == -1:\n",
    "                avg_cp += (len(data[s_id][g_id]['y_pred']) + 1) / len(data[s_id][g_id]['y_pred'])\n",
    "            else:\n",
    "                avg_cp += cp / len(data[s_id][g_id]['y_pred'])\n",
    "            data[s_id][g_id]['accuracy'] /= len(data[s_id][g_id]['y_pred'])\n",
    "            accuracies.append(data[s_id][g_id]['accuracy'])\n",
    "        std_cp += avg_cp / len(data[s_id])\n",
    "        mean_accuracy += mean(accuracies)\n",
    "    \n",
    "    return {'mean_accuracy': mean_accuracy/len(data), 'std_cp': std_cp/len(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_scores(y_pred, y_true, joint_ids):\n",
    "    accuracy = 0\n",
    "    ratings = {'correct': [], 'incorrect': []}\n",
    "    for y_idx in range(len(y_pred)):\n",
    "        if np.argmax(y_pred[y_idx]) == np.argmax(y_true[y_idx]):\n",
    "            accuracy += 1\n",
    "            rating = float(joint_ids['prev_responses'][y_idx]['rating'])\n",
    "            if rating != -1:\n",
    "                ratings['correct'].append(rating)\n",
    "        else:\n",
    "            rating = float(joint_ids['prev_responses'][y_idx]['rating'])\n",
    "            if rating != -1:\n",
    "                ratings['incorrect'].append(rating)\n",
    "            #print(joint_ids['prev_responses'][y_idx]['response'])\n",
    "            \n",
    "    print('pos_rating_avg: ', mean(ratings['correct']), stdev(ratings['correct']),\n",
    "          '\\tneg_rating_avg: ', mean(ratings['incorrect']), stdev(ratings['incorrect']))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(ratings.values())\n",
    "    ax.set_xticklabels(ratings.keys())\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return {'accuracy': accuracy / len(y_pred)}, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_results(y_pred, prev_goals):\n",
    "    for idx in range(len(y_pred)):\n",
    "        while fewer_plot_names[np.argmax(y_pred[idx])] in prev_goals:\n",
    "            if prev_goals.index(fewer_plot_names[np.argmax(y_pred[idx])]) != len(prev_goals)-1:\n",
    "                y_pred[idx][np.argmax(y_pred[idx])] = 0\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test_CV(data, y_pred, y_true, joint_ids, set_name, seq_length=20, \n",
    "                        extension=\"\", inspect_bias=True):\n",
    "    y_pred = post_process_results(y_pred, joint_ids['prev_goals'])\n",
    "    goals_dict = create_dict(y_pred, y_true, joint_ids)\n",
    "    micro_results, ratings = get_micro_scores(y_pred, y_true, joint_ids)\n",
    "    \n",
    "    result_metrics = {'goals_macro': get_macro_scores(goals_dict),\n",
    "                      'goals_micro': micro_results}\n",
    "    \n",
    "    if set_name == 'TEST':\n",
    "        return result_metrics, ratings\n",
    "    \n",
    "    return result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
