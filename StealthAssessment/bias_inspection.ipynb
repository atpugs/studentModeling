{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_cols = {2018: 17, 2019: 18}\n",
    "protected_cols = {'Gender': {2018: 18, 2019: 20}, \n",
    "                  'Prior_exp': {2018: 24, 2019: 25}}\n",
    "pretest_filenames = {2018: \"Datasets/2018_REFLECT_Pretest.csv\",\n",
    "                     2019: \"Datasets/2019_REFLECT_Pretest.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta44/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from build_models.ipynb\n",
      "importing Jupyter notebook from preprocessing.ipynb\n",
      "importing Jupyter notebook from fetch_data.ipynb\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from evaluation.ipynb\n",
      "importing Jupyter notebook from bias_mitigation.ipynb\n"
     ]
    }
   ],
   "source": [
    "from abroca import *\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, precision_recall_fscore_support, accuracy_score\n",
    "from statistics import mean, stdev\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import import_ipynb\n",
    "from build_models import *\n",
    "from evaluation import *\n",
    "from bias_mitigation import bias_postprocessing\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_gender(data):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_exp(data):\n",
    "    for s_id in data:\n",
    "        if data[s_id]['Prior_exp'] == '2':\n",
    "            data[s_id]['Prior_exp'] == '1'\n",
    "        elif data[s_id]['Prior_exp'] == '3' or data[s_id]['Prior_exp'] == '4' or data[s_id]['Prior_exp'] == '5':\n",
    "            data[s_id]['Prior_exp'] = '2'\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protected_attr(data, protected_label=\"Gender\"):\n",
    "    studies = [2018, 2019]\n",
    "    for s_id in data:\n",
    "        found = False\n",
    "        for year in studies:\n",
    "            with open(pretest_filenames[year]) as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                for row in csv_reader:\n",
    "                    if row[username_cols[year]] == s_id or row[username_cols[year]] in s_id:\n",
    "                        found = True\n",
    "                        data[s_id][protected_label] = row[protected_cols[protected_label][year]]\n",
    "            if found == True:\n",
    "                break\n",
    "                \n",
    "    if protected_label == \"Gender\":\n",
    "        reassign_gender(data)\n",
    "    elif protected_label == \"Prior_exp\":\n",
    "        reassign_exp(data)\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protected_attr_by_id(ids, protected_label=\"Gender\"):\n",
    "    studies = [2018, 2019]\n",
    "    attrs = {}\n",
    "    for s_id in ids:\n",
    "        found = False\n",
    "        for year in studies:\n",
    "            with open(pretest_filenames[year]) as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                for row in csv_reader:\n",
    "                    if row[username_cols[year]] == s_id or row[username_cols[year]] in s_id:\n",
    "                        found = True\n",
    "                        attrs[s_id] = {'Gender': '-1', 'Prior_exp': '-1'}\n",
    "                        attrs[s_id][protected_label] = row[protected_cols[protected_label][year]]\n",
    "            if found == True:\n",
    "                break\n",
    "                \n",
    "    if protected_label == \"Gender\":\n",
    "        reassign_gender(attrs)\n",
    "    elif protected_label == \"Prior_exp\":\n",
    "        reassign_exp(attrs)\n",
    "                \n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data, protected_label=\"Gender\", label_name=\"posttest\"):\n",
    "    new_dict = {'ids': [], protected_label: [], 'y_pred': [], 'y_true': []}\n",
    "    for s_id in data:\n",
    "        count = len(data[s_id]['y_pred'])\n",
    "        #new_dict[protected_label].extend([data[s_id][protected_label] for i in range(count)])\n",
    "        new_dict[protected_label].append(data[s_id][protected_label])\n",
    "        #new_dict['ids'].extend([s_id for i in range(count)])\n",
    "        new_dict['ids'].append(s_id)\n",
    "        if label_name == \"posttest\":\n",
    "            #new_dict['y_pred'].extend([y for y in data[s_id]['y_pred']])\n",
    "            #new_dict['y_true'].extend([y>=0.43 for y in data[s_id]['y_true']])\n",
    "            new_dict['y_pred'].append(mean(data[s_id]['y_pred']))\n",
    "            m = 0\n",
    "            if data[s_id]['y_true'][0] > 6/17:\n",
    "                m = 1\n",
    "            new_dict['y_true'].append(m)\n",
    "        elif label_name == \"ratings\":\n",
    "            #new_dict['y_pred'].extend([y for y in data[s_id]['y_pred']])\n",
    "            #new_dict['y_true'].extend([y>=0.5 for y in data[s_id]['y_true']])\n",
    "            new_dict['y_pred'].append(mean(data[s_id]['y_pred']))\n",
    "            m = 0\n",
    "            if data[s_id]['y_true'][0] >= 2.5/5:\n",
    "                m = 1\n",
    "            new_dict['y_true'].append(m)\n",
    "            \n",
    "    print(len(new_dict['y_pred']), len(new_dict['y_true']))\n",
    "        \n",
    "    return pd.DataFrame(data=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(data, protected_label='Gender', value=1, label_name='posttest', disp_ax=None):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    instance_count = 0\n",
    "    for s_id in data:\n",
    "        if data[s_id][protected_label] == str(value):\n",
    "            instance_count += 1\n",
    "            if label_name == \"posttest\":\n",
    "                #y_pred.extend([y for y in data[s_id]['y_pred']])\n",
    "                #y_true.extend([y>=0.43 for y in data[s_id]['y_true']])\n",
    "                y_pred.append(mean(data[s_id]['y_pred']))\n",
    "                m = 0\n",
    "                if data[s_id]['y_true'][0] > 6/17:\n",
    "                    m = 1\n",
    "                y_true.append(m)\n",
    "            elif label_name == \"ratings\":\n",
    "                #y_pred.extend([y for y in data[s_id]['y_pred']])\n",
    "                #y_true.extend([y>=0.5 for y in data[s_id]['y_true']])\n",
    "                y_pred.append(mean(data[s_id]['y_pred']))\n",
    "                m = 0\n",
    "                if data[s_id]['y_true'][0] >= 2.5/5:\n",
    "                    m = 1\n",
    "                y_true.append(m)\n",
    "      \n",
    "    print('Instance count: ', instance_count)\n",
    "    print(protected_label, \"[Value: \", value, \" AUC: \", roc_auc_score(y_true, y_pred), \"]\")\n",
    "    \n",
    "    legend = {'Gender': {1: 'Male ROC', 2: 'Female ROC'}, \n",
    "              'Prior_exp': {1: '0-5hrs ROC', 2: '>5hrs ROC'}}\n",
    "    name = legend[protected_label][value]\n",
    "    \n",
    "    if disp_ax != None:\n",
    "        return RocCurveDisplay.from_predictions(y_true, y_pred, name=name, ax=disp_ax)\n",
    "    \n",
    "    return RocCurveDisplay.from_predictions(y_true, y_pred, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bias(data, label_name=\"posttest\", protected_label=\"Gender\"):\n",
    "    data = get_protected_attr(data, protected_label=protected_label) \n",
    "    df = create_dataframe(data, protected_label=protected_label, label_name=label_name)\n",
    "    \n",
    "    #abroca_results = compute_abroca(df, pred_col='y_pred', label_col='y_true',\n",
    "    #                                protected_attr_col=protected_label, majority_protected_attr_val=1,\n",
    "    #                                #compare_type='multiple', \n",
    "    #                                plot_slices=True)\n",
    "    #print(\"ABROCA results for \", label_name, \" [\", protected_label, \"]: \\t\", abroca_results)\n",
    "    \n",
    "    if protected_label == 'Gender':\n",
    "        disp1 = compute_auc(data, protected_label='Gender', value=1, label_name=label_name)\n",
    "        disp2 = compute_auc(data, protected_label='Gender', value=2, label_name=label_name,\n",
    "                            disp_ax=disp1.ax_)\n",
    "        #compute_auc(data, protected_label='Gender', value=3, label_name=label_name)\n",
    "    elif protected_label == 'Prior_exp':\n",
    "        disp1 = compute_auc(data, protected_label='Prior_exp', value=1, label_name=label_name)\n",
    "        compute_auc(data, protected_label='Prior_exp', value=2, label_name=label_name, \n",
    "                    disp_ax=disp1.ax_)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_bias(model_filename, test_data, get_required_model, label_name=\"posttest\", protected_label=\"Gender\",\n",
    "                   postprocess=False, algorithm='CalibratedEqOddsPostprocessing'):\n",
    "    model_trained = load_model(model_filename)\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    y_pred = model_trained.predict(data['Test'])\n",
    "    y_true = labels['Test']\n",
    "    \n",
    "    y_pred = [[label1[0] for label1 in y_pred[0]], [label2[0] for label2 in y_pred[1]]]\n",
    "    \n",
    "    if postprocess:\n",
    "        y_pred[1] = bias_postprocessing(data['Test'], y_true, y_pred[1], ids['Test'], algorithm=algorithm)\n",
    "        \n",
    "    posttest_dict = create_dict(y_pred[0], y_true[0], ids['Test']['ids']) \n",
    "    #ratings_dict1 = create_dict(y_pred[1], y_true[1], ids['Test']['ids'])\n",
    "    ratings_dict = create_dict(y_pred[1], y_true[1], ids['Test']['response_ids'])\n",
    "    \n",
    "    get_accuracy(posttest_dict, threshold=5/17, label=\"Posttest\")\n",
    "    get_accuracy(ratings_dict, threshold=2.5/5, label=\"Ratings\")\n",
    "    \n",
    "    print(\"ABROCA results for overall set\")\n",
    "    if label_name == 'posttest':\n",
    "        detect_bias(posttest_dict, label_name=label_name, protected_label=protected_label) \n",
    "    elif label_name == 'ratings':\n",
    "        detect_bias(ratings_dict, label_name=label_name, protected_label=protected_label) \n",
    "    \n",
    "    result_metrics = {'posttest_macro': get_macro_scores(posttest_dict), \n",
    "                      'rating_macro': get_macro_scores(ratings_dict),\n",
    "                      'posttest_micro': get_micro_scores(y_pred[0], y_true[0]), \n",
    "                      'ratings_micro': get_micro_scores(y_pred[1], y_true[1])}\n",
    "    \n",
    "    print(\"OVERALL SET: \", result_metrics)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gametrace_stats(test_data, get_required_model, protected_label=\"Gender\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': [[] for i in range(8)], '2': [[] for i in range(8)]}\n",
    "    \n",
    "    for i,s_id in enumerate(ids):\n",
    "        gender = genders[s_id][protected_label]\n",
    "        if gender in gender_logs:\n",
    "            for idx in range(8):\n",
    "                gender_logs[gender][idx].append(logs[i][idx])\n",
    "                \n",
    "    for gender in gender_logs:\n",
    "        #gender_logs[gender] = [n/len(ids) for n in gender_logs[gender]]\n",
    "        print(gender)\n",
    "        stdevs = []\n",
    "        means = []\n",
    "        for idx in range(8):\n",
    "            #print(idx, stdev(gender_logs[gender][idx]))\n",
    "            #print(idx, mean(gender_logs[gender][idx]))\n",
    "            stdevs.append(stdev(gender_logs[gender][idx])*stdev(gender_logs[gender][idx]))\n",
    "            means.append(stdev(gender_logs[gender][idx])*stdev(gender_logs[gender][idx]))\n",
    "        print(\"mean: \", mean(means), \" stdev: \", stdev(stdevs))\n",
    "        \n",
    "    gender_logs = {'1': [], '2': []}\n",
    "    \n",
    "    for i,s_id in enumerate(ids):\n",
    "        gender = genders[s_id][protected_label]\n",
    "        if gender in gender_logs:\n",
    "            gender_logs[gender].extend(logs[i])\n",
    "            \n",
    "    for gender in gender_logs:\n",
    "        print(gender)\n",
    "        print(\"Mean: \", mean(gender_logs[gender]))\n",
    "        print(\"Stdev: \", stdev(gender_logs[gender]))\n",
    "    \n",
    "    return #gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_r_stats(test_data, get_required_model, protected_label=\"Gender\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': []}, '2': {'Ratings': [], 'Readings': []}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = []\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Gender'] = gender\n",
    "        if response_ids[i] != s_id + str(r_id):\n",
    "            print(i, response_ids[i])\n",
    "            reflect_length, response = get_reflection_length(s_id, r_id)\n",
    "            if reflect_length != 0:\n",
    "                data_sid[s_id]['Ratings'].append(labels['Train'][1][i-1])\n",
    "                data_sid[s_id]['Readings'].append(reflect_length)\n",
    "            r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        if data_sid[s_id]['Gender'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_gametrace_stats(test_data, get_required_model, protected_label=\"Gender\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': []}, '2': {'Ratings': [], 'Readings': []}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = []\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Gender'] = gender\n",
    "        if response_ids[i] != s_id + str(r_id):\n",
    "            print(i, response_ids[i])\n",
    "            data_sid[s_id]['Ratings'].append(labels['Train'][1][i-1])\n",
    "            data_sid[s_id]['Readings'].append(logs[i-1][1] + logs[i-1][5])\n",
    "            r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        if data_sid[s_id]['Gender'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretest_reflect_stats(test_data, get_required_model, protected_label=\"Prior_exp\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': [], 'low': 0, 'high': 0}, \n",
    "                   '2': {'Ratings': [], 'Readings': [], 'low': 0, 'high': 0}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            rating = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = [(labels['Train'][0][i]-data['Train'][2][i])/(1-data['Train'][2][i])]\n",
    "            data_sid[s_id]['Ratings'] = [data['Train'][2][i]]\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Prior_exp'] = gender\n",
    "        rating, response = get_reflection_length(s_id, int(response_ids[i][len(response_ids[i])-1]))\n",
    "        if rating != 0:\n",
    "            data_sid[s_id]['Readings'].append(rating)\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        try:\n",
    "            data_sid[s_id]['Readings'] = [mean(data_sid[s_id]['Readings'])]\n",
    "        except:\n",
    "            data_sid[s_id]['Readings'] = [0]\n",
    "        data_sid[s_id]['Prior_exp'] = '1'\n",
    "        if data_sid[s_id]['Prior_exp'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "            if mean(data_sid[s_id]['Readings']) >= 0.5:\n",
    "                gender_logs[data_sid[s_id]['Prior_exp']]['high'] += 1\n",
    "            else:\n",
    "                gender_logs[data_sid[s_id]['Prior_exp']]['low'] += 1\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "        gender_logs[gender]['mean'] = mean(gender_logs[gender]['Readings'])\n",
    "        gender_logs[gender]['variance'] = stdev(gender_logs[gender]['Readings'])\n",
    "        print(gender_logs[gender]['p'], gender_logs[gender]['mean'], gender_logs[gender]['variance'],\n",
    "              gender_logs[gender]['low'], gender_logs[gender]['high'])\n",
    "        print(stats.ttest_ind(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings']))\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_reflect_stats(test_data, get_required_model, protected_label=\"Prior_exp\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': [], 'low': 0, 'high': 0}, \n",
    "                   '2': {'Ratings': [], 'Readings': [], 'low': 0, 'high': 0}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            rating = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = [(labels['Train'][0][i]-data['Train'][2][i])/(1-data['Train'][2][i])]\n",
    "            data_sid[s_id]['Ratings'] = [labels['Train'][0][i]]\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Prior_exp'] = gender\n",
    "        rating, response = get_reflection_length(s_id, int(response_ids[i][len(response_ids[i])-1]))\n",
    "        if rating != 0:\n",
    "            data_sid[s_id]['Readings'].append(rating)\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        try:\n",
    "            data_sid[s_id]['Readings'] = [mean(data_sid[s_id]['Readings'])]\n",
    "        except:\n",
    "            data_sid[s_id]['Readings'] = [0]\n",
    "        if data_sid[s_id]['Prior_exp'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "            if mean(data_sid[s_id]['Readings']) >= 0.5:\n",
    "                gender_logs[data_sid[s_id]['Prior_exp']]['high'] += 1\n",
    "            else:\n",
    "                gender_logs[data_sid[s_id]['Prior_exp']]['low'] += 1\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "        gender_logs[gender]['mean'] = mean(gender_logs[gender]['Readings'])\n",
    "        gender_logs[gender]['variance'] = stdev(gender_logs[gender]['Readings'])\n",
    "        print(gender_logs[gender]['p'], gender_logs[gender]['mean'], gender_logs[gender]['variance'],\n",
    "              gender_logs[gender]['low'], gender_logs[gender]['high'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_reflection_stats(test_data, get_required_model, protected_label=\"Prior_exp\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    response_embs = [n[19] for n in data['Train'][1]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Reflect_length': [], 'low': 0, 'high': 0}, \n",
    "                   '2': {'Ratings': [], 'Reflect_length': [], 'low': 0, 'high': 0}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = int(response_ids[i][len(response_ids[i])-1])\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = []\n",
    "            data_sid[s_id]['Reflect_length'] = []\n",
    "            data_sid[s_id]['Prior_exp'] = gender\n",
    "        if response_ids[i] != s_id + str(r_id):\n",
    "            print(i, response_ids[i])\n",
    "            reflect_length, response = get_reflection_length(s_id, r_id)\n",
    "            if reflect_length != 0:\n",
    "                if reflect_length != 1:\n",
    "                    data_sid[s_id]['Ratings'].append((labels['Train'][1][i-1]-reflect_length)/(1-reflect_length))\n",
    "                else:\n",
    "                    data_sid[s_id]['Ratings'].append((labels['Train'][1][i-1]-reflect_length)/reflect_length)\n",
    "                data_sid[s_id]['Reflect_length'].append(reflect_length)\n",
    "            r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        if data_sid[s_id]['Prior_exp'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Reflect_length'].extend(data_sid[s_id]['Reflect_length'])\n",
    "            try:\n",
    "                if mean(data_sid[s_id]['Reflect_length']) >= 0.5:\n",
    "                    gender_logs[data_sid[s_id]['Prior_exp']]['high'] += 1\n",
    "                else:\n",
    "                    gender_logs[data_sid[s_id]['Prior_exp']]['low'] += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Reflect_length'], gender_logs[gender]['Ratings'])\n",
    "        gender_logs[gender]['mean'] = mean(gender_logs[gender]['Reflect_length'])\n",
    "        gender_logs[gender]['variance'] = stdev(gender_logs[gender]['Reflect_length'])\n",
    "        print(gender_logs[gender]['p'], gender_logs[gender]['mean'], gender_logs[gender]['variance'],\n",
    "              gender_logs[gender]['low'], gender_logs[gender]['high'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_gametrace_stats(test_data, get_required_model, protected_label=\"Prior_exp\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': []}, '2': {'Ratings': [], 'Readings': []}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = [labels['Train'][0][i]]\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Prior_exp'] = gender\n",
    "        data_sid[s_id]['Readings'].append(logs[i-1][1] + logs[i-1][5])\n",
    "        r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        data_sid[s_id]['Readings'] = [mean(data_sid[s_id]['Readings'])]\n",
    "        if data_sid[s_id]['Prior_exp'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_reflection_stats(test_data, get_required_model, protected_label=\"Gender\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    response_embs = [n[19] for n in data['Train'][1]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Reflect_length': [], 'Reflections': []}, \n",
    "                   '2': {'Ratings': [], 'Reflect_length': [], 'Reflections': []}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = int(response_ids[i][len(response_ids[i])-1])\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = []\n",
    "            data_sid[s_id]['Reflect_length'] = []\n",
    "            data_sid[s_id]['Gender'] = gender\n",
    "        if response_ids[i] != s_id + str(r_id):\n",
    "            print(i, response_ids[i])\n",
    "            reflect_length = get_reflection_length(s_id, r_id)\n",
    "            if reflect_length != 0:\n",
    "                data_sid[s_id]['Ratings'].append(labels['Train'][1][i-1])\n",
    "                data_sid[s_id]['Reflect_length'].append(reflect_length)\n",
    "            r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        if data_sid[s_id]['Gender'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Reflect_length'].extend(data_sid[s_id]['Reflect_length'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Reflect_length'], gender_logs[gender]['Ratings'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reflection_length(s_id, r_id):\n",
    "    with open('Datasets/LabeledReflections.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] == s_id and row[1] == str(r_id):\n",
    "                return len(re.findall(r'\\w+', row[5])), row[5]\n",
    "                #eturn float(row[8])/5, row[5]\n",
    "    \n",
    "    print(\"ALERT\")\n",
    "    return 0, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_reflection_stats(test_data, get_required_model, protected_label=\"Gender\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    response_embs = [n[19] for n in data['Train'][1]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Reflect_length': [], 'Reflections': []}, \n",
    "                   '2': {'Ratings': [], 'Reflect_length': [], 'Reflections': []}}\n",
    "    data_sid = {}\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = int(response_ids[i][len(response_ids[i])-1])\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = []\n",
    "            data_sid[s_id]['Reflect_length'] = []\n",
    "            data_sid[s_id]['Reflections'] = []\n",
    "            data_sid[s_id]['Gender'] = gender\n",
    "        if response_ids[i] != s_id + str(r_id):\n",
    "            print(i, response_ids[i])\n",
    "            reflect_length, response = get_reflection_length(s_id, r_id)\n",
    "            if reflect_length != 0:\n",
    "                data_sid[s_id]['Ratings'].append(labels['Train'][0][i-1])\n",
    "                data_sid[s_id]['Reflect_length'].append(reflect_length)\n",
    "                data_sid[s_id]['Reflections'].append(response)\n",
    "            r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        if data_sid[s_id]['Gender'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Reflect_length'].extend(data_sid[s_id]['Reflect_length'])\n",
    "            gender_logs[data_sid[s_id]['Gender']]['Reflections'].extend(data_sid[s_id]['Reflections'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Reflect_length'], gender_logs[gender]['Ratings'])\n",
    "        bin_ratings = [r>2.5 for r in gender_logs[gender]['Ratings']]\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, threshold=0.5, label=\"Posttest\"):\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for s_id in data:\n",
    "        pred_labels.append(mean(data[s_id]['y_pred']) >= threshold)\n",
    "        true_labels.append(mean(data[s_id]['y_true']) >= threshold)\n",
    "            \n",
    "    print(label, \"accuracy: \", accuracy_score(true_labels, pred_labels))\n",
    "    print(precision_recall_fscore_support(true_labels, pred_labels, average=None))\n",
    "    print(precision_recall_fscore_support(true_labels, pred_labels, average='micro'))\n",
    "    print(\"AUC: \", roc_auc_score(true_labels, pred_labels))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_game_stats(test_data, get_required_model, protected_label=\"Prior_exp\"):\n",
    "    raw_data = {'Train': deepcopy(test_data),\n",
    "                'Val': deepcopy(test_data),\n",
    "                'Test': deepcopy(test_data)}\n",
    "    model, data, labels, ids = get_required_model(raw_data, seq_length=20)\n",
    "    logs = [n[19] for n in data['Train'][0]]\n",
    "    print(\"got logs\")\n",
    "    response_ids = ids['Train']['response_ids']\n",
    "    ids = ids['Train']['ids']\n",
    "    genders = get_protected_attr_by_id(ids, protected_label=protected_label)\n",
    "    gender_logs = {'1': {'Ratings': [], 'Readings': []}, '2': {'Ratings': [], 'Readings': []}}\n",
    "    data_sid = {}\n",
    "    durations = get_durations()\n",
    "    \n",
    "    for i, s_id in enumerate(durations):\n",
    "        if s_id not in ids:\n",
    "            continue\n",
    "        if s_id not in data_sid:\n",
    "            data_sid[s_id] = {}\n",
    "            r_id = 0\n",
    "            gender = genders[s_id][protected_label]\n",
    "            data_sid[s_id]['Ratings'] = [labels['Train'][0][ids.index(s_id)]]\n",
    "            data_sid[s_id]['Readings'] = []\n",
    "            data_sid[s_id]['Prior_exp'] = gender\n",
    "            data_sid[s_id]['Readings'] = durations[s_id]['reading_durations']\n",
    "        r_id += 1\n",
    "            \n",
    "    for s_id in data_sid:\n",
    "        data_sid[s_id]['Readings'] = [mean(data_sid[s_id]['Readings'])]\n",
    "        if data_sid[s_id]['Prior_exp'] in gender_logs:\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Ratings'].extend(data_sid[s_id]['Ratings'])\n",
    "            gender_logs[data_sid[s_id]['Prior_exp']]['Readings'].extend(data_sid[s_id]['Readings'])\n",
    "        \n",
    "    for gender in gender_logs:\n",
    "        gender_logs[gender]['p'] = pearsonr(gender_logs[gender]['Readings'], gender_logs[gender]['Ratings'])\n",
    "    \n",
    "    return gender_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations():\n",
    "    data = {}\n",
    "    event_files = ['Datasets/EventSequence2018.csv', 'Datasets/EventSequence2019.csv']\n",
    "    \n",
    "    for filename in event_files:\n",
    "        with open(filename) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            row_ct = 0\n",
    "            for row in csv_reader:\n",
    "                if row_ct == 0:\n",
    "                    row_ct += 1\n",
    "                    continue\n",
    "                if row[0] not in data:\n",
    "                    data[row[0]] = {'reading_durations': [], 'cumulative_duration': 0}\n",
    "                if row[1] == 'BooksAndArticles' or row[1] == 'Posters':# or row[1] == 'Conversation': \n",
    "                    try:\n",
    "                        data[row[0]]['cumulative_duration'] += float(row[3])\n",
    "                    except:\n",
    "                        pass\n",
    "                    data[row[0]]['reading_durations'].append(data[row[0]]['cumulative_duration'])\n",
    "                    \n",
    "                    \n",
    "    for s_id in data:\n",
    "        data[s_id]['mean_duration'] = sum(data[s_id]['reading_durations'])\n",
    "        data[s_id]['stdev_duration'] = stdev(data[s_id]['reading_durations'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reflection_attempts(data):\n",
    "    counts = []\n",
    "    for s_id in data:\n",
    "        try:\n",
    "            counts.append(data[s_id]['logs'][len(data[s_id]['logs'])-1]['response_number'] + 1)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    print(mean(counts), stdev(counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
