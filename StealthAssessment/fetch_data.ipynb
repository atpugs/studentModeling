{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of columns in csv file\n",
    "columns = ['TestSubject', 'Event', 'TimeStamp', 'Duration', 'Location',\n",
    "          'GameTime', 'Target', 'HowIsItGoingLikert', 'ProgressPlanSummary',\n",
    "          'SolutionApproachSummary', 'DifferentProblemApproachSummary',\n",
    "          'Name', 'Title', 'NPC', 'NpcSpokeCount', 'ConceptMatrixEditCount',\n",
    "          'ConceptMatrixAnsweredCorrectly', 'ObjectScanned', 'TestingFor',\n",
    "          'ReasonForTesting', 'TotalFieldsModified', 'WorksheetSubmitResult']\n",
    "\n",
    "# Order of columns in activity summary csv file\n",
    "activity_columns = ['TestSubject', 'ActivityUri', 'ActivityStarts', 'ActivityRestarts',\n",
    "                    'Age', 'Gender', 'Race', 'VideoGamePlayingFrequency', \n",
    "                    'VideoGamePlayingSkill', 'VideoGamePlayingHoursPerWeek', 'VideoGamesPlayed',\n",
    "                    'PreTestScore', 'PostTestScore', 'LearningGain', 'MysterySolved', \n",
    "                    'TotalPromptOpens', 'MeanPromptResponseDuration', 'MeanHowIsItGoingLikert',\n",
    "                    'TotalWorksheetSubmits', 'SolutionDisease', 'InfectionType', 'SolutionObject',\n",
    "                    'SolutionTreatment', 'StartTime', 'StopTime', 'Duration', 'PlotPointsActivated',\n",
    "                    'TotalComplexTextDuration', 'MeanComplexTextDuration', 'TotalPosterOpenDuration',\n",
    "                    'MeanPosterOpenDuration', 'TotalScanningDuration', 'MeanScanningDuration', \n",
    "                    'TotalDiagnosisWorksheetDuration', 'MeanDiagnosisWorksheetDuration',\n",
    "                    'TotalDialogSelections', 'TotalComplexTextOpens', 'TotalPostersOpens', 'TotalScans',\n",
    "                    'TotalDiagnosisWorksheetOpens', 'TotalBackpackOpens']\n",
    "\n",
    "# Order of columns in reflect data csv file\n",
    "reflect_columns = ['StudentId', 'Instance', 'PromptId', 'GameTime(sec)',\n",
    "                  'QuestionId', 'Response', 'ReflectionRating1', 'ReflectionRating2',\n",
    "                  'ReflectionRatingAvg', 'PreTestScore', 'PostTestScore', \n",
    "                  'NormalizedLearningGain', 'NLG_revised']\n",
    "\n",
    "events_without_movement = ['Conversation', 'BooksAndArticles', 'Worksheet', 'Prompts', 'PlotPoint',\n",
    "                           'Posters', 'Scanner', 'WorksheetSubmit']\n",
    "\n",
    "plot_names = ['IntroFromKim', 'TutorialComplete', 'CompletedGOT', 'TalkedToBryce', \n",
    "              'SecondaryPatientSymptoms', 'TalkedToQuentin', 'LearnFoodHistory', \n",
    "              'TalkedToElise', 'TestObject', 'TalkedToTheresa', 'PrimaryPatientSymptoms', \n",
    "              'TalkedToSam', 'TalkedToGreg', 'TestContaminatedObject', 'SubmittedDiagnosis', \n",
    "              'TalkedToRobert', 'LearnAboutBacteria', 'SolvedMystery', 'TalkedToFord', \n",
    "              'LearnAboutViruses'] \n",
    "\n",
    "filenames = {2018:\"Datasets/EventSequence2018.csv\", 2019:\"Datasets/EventSequence2019.csv\"}\n",
    "\n",
    "summary_filenames = {2018:\"Datasets/ActivitySummary2018.csv\", 2019:\"Datasets/ActivitySummary2019.csv\"}\n",
    "\n",
    "# Reflection data csv filename\n",
    "reflect_file = 'Datasets/LabeledReflections.csv'\n",
    "\n",
    "elmo_module_locn = \"module/module_elmo2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_elmo2(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "\n",
    "elmo = hub.load(elmo_module_locn)\n",
    "embed_fn = embed_elmo2(elmo_module_locn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, pca=None, pca_dim=32):\n",
    "    if text != \"\": # string not blank\n",
    "        embeddings = elmo.signatures['default'](tf.convert_to_tensor([text]))[\"elmo\"]\n",
    "    else: # return default embedding\n",
    "        if pca == None: \n",
    "            return [random.uniform(-1,1) for i in range(emb_dim)]\n",
    "        return [random.uniform(-1,1) for i in range(pca_dim)]\n",
    "    \n",
    "    x = embed_fn([text]) # get elmo embedding of string (sentence-level)\n",
    "    embed = x[0].tolist() # there will be only one vector (like, [[vector]]), get rid of extra\n",
    "                          # dimension\n",
    "    if pca == None: # if no PCA model is provided, return embedding directly\n",
    "        return embed\n",
    "    \n",
    "    return pca.transform([embed])[0] # transform embedding and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(s_id, study_number):\n",
    "    pretest, posttest = -1, -1 # entries with pretest/posttest -1 are ignored in final dataset\n",
    "    student_found = False\n",
    "    \n",
    "    for study in summary_filenames:\n",
    "        with open(summary_filenames[study]) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row in csv_reader:\n",
    "                if row[activity_columns.index('TestSubject')] == s_id:\n",
    "                    student_found = True\n",
    "                    pretest_txt = row[activity_columns.index('PreTestScore')]\n",
    "                    posttest_txt = row[activity_columns.index('PostTestScore')]\n",
    "                    if pretest_txt != \"\" and posttest_txt != \"\":\n",
    "                        pretest = float(pretest_txt)\n",
    "                        posttest = float(posttest_txt)\n",
    "                        break\n",
    "        if student_found:\n",
    "            break\n",
    "                    \n",
    "    return pretest, posttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_rating(s_id, response_number):\n",
    "    next_rating = -1 # entries with next rating -1 are ignored in final dataset\n",
    "    \n",
    "    with open(reflect_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            id_idx = reflect_columns.index('StudentId')\n",
    "            instance_idx = reflect_columns.index('Instance')\n",
    "            rating_idx = reflect_columns.index('ReflectionRatingAvg')\n",
    "            if row[id_idx] == s_id and float(row[instance_idx]) == response_number + 1:\n",
    "                next_rating = row[rating_idx]\n",
    "                \n",
    "    return next_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs(s_id, data_row, prev_log, pca=None, pca_dim=32, tutorial_complete=False):\n",
    "    log = {}\n",
    "    \n",
    "    event_vec = [0 for e in range(len(events_without_movement))]\n",
    "    goal_vec = [0 for g in range(len(plot_names))]\n",
    "    goal_vec[plot_names.index('IntroFromKim')] = 1\n",
    "    goal_vec[plot_names.index('TutorialComplete')] = 1\n",
    "    emb = [random.uniform(-1,1) for e in range(pca_dim)]\n",
    "    next_rating = -1\n",
    "    response_number = 0\n",
    "    if prev_log: \n",
    "        # if previous action exists, maintain prev goals, response emb, next rating\n",
    "        # and current response number\n",
    "        goal_vec = deepcopy(prev_log['goal'])\n",
    "        emb = deepcopy(prev_log['response_emb'])\n",
    "        next_rating = deepcopy(prev_log['next_rating'])\n",
    "        response_number = deepcopy(prev_log['response_number'])\n",
    "    \n",
    "    event_col = columns.index('Event')\n",
    "    if data_row[event_col] in events_without_movement:\n",
    "        event_vec[events_without_movement.index(data_row[event_col])] = 1 # set current event to 1\n",
    "    else:\n",
    "        return {}, tutorial_complete # event is movement, and hence is not added to logs\n",
    "    \n",
    "    if data_row[event_col] == 'PlotPoint': # if new goal achieved\n",
    "        plot_col = columns.index('Name')\n",
    "        goal_vec[plot_names.index(data_row[plot_col])] = 1 # set new goal to 1\n",
    "        if data_row[plot_col] == 'TutorialComplete':\n",
    "            tutorial_complete = True\n",
    "            \n",
    "    if data_row[event_col] == 'Prompts':\n",
    "        progress_plan_col = columns.index('ProgressPlanSummary')\n",
    "        solution_col = columns.index('SolutionApproachSummary')\n",
    "        diff_solution_col = columns.index('DifferentProblemApproachSummary')\n",
    "        # combine all response types to construct response\n",
    "        response = data_row[progress_plan_col] + data_row[solution_col] + data_row[diff_solution_col]\n",
    "        emb = get_embedding(response, pca=pca, pca_dim=pca_dim)\n",
    "        response_number += 1 # update current response count for student\n",
    "        next_rating = get_next_rating(s_id, response_number) # get response rating for next response\n",
    "    elif next_rating == -1:\n",
    "        next_rating = get_next_rating(s_id, response_number) # get response rating for next response\n",
    "    \n",
    "    if prev_log:\n",
    "        log['event'] = [a + b for a, b in zip(prev_log['event'], event_vec)] # event vector is a count vec\n",
    "    else:\n",
    "        log['event'] = event_vec\n",
    "    log['goal'] = goal_vec\n",
    "    log['response_emb'] = emb\n",
    "    log['next_rating'] = next_rating\n",
    "    log['response_number'] = response_number\n",
    "    \n",
    "    return log, tutorial_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_model(pca_dim):\n",
    "    pca = PCA(pca_dim)\n",
    "    \n",
    "    responses = []\n",
    "    with open(reflect_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i,row in enumerate(csv_reader): # for each written response in LabeledReflections file\n",
    "            if i == 0: # ignore column headers\n",
    "                continue\n",
    "            idx = reflect_columns.index('Response') # get column index of response\n",
    "            if row[idx] != \"\": # if response exists (not blank)\n",
    "                responses.append(get_embedding(row[idx])) # append ELMo embedding\n",
    "    \n",
    "    pca.fit(responses)\n",
    "    \n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_logs(pca_dim=32, movement=False):\n",
    "    # get PCA model for response embeddings\n",
    "    pca = get_pca_model(pca_dim)\n",
    "    \n",
    "    data = {} # dictionary to store student logs [Key: StudentID]\n",
    "    \n",
    "    for study in filenames: # study: 2018/2019\n",
    "        with open(filenames[study]) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row_idx, row in enumerate(csv_reader):\n",
    "                if row_idx == 0: # skip header\n",
    "                    continue\n",
    "                \n",
    "                s_id = row[columns.index('TestSubject')] # ID of current student\n",
    "                if s_id not in data: # add student to dictionary\n",
    "                    data[s_id] = {}\n",
    "                    pretest, posttest = get_scores(s_id, study)\n",
    "                    data[s_id]['pretest'] = pretest\n",
    "                    data[s_id]['posttest'] = posttest\n",
    "                    data[s_id]['tutorial_complete'] = False\n",
    "                    data[s_id]['logs'] = [] # no actions logged yet\n",
    "                    \n",
    "                if data[s_id]['posttest'] != -1 and data[s_id]['pretest'] != -1:\n",
    "                    if len(data[s_id]['logs']) > 0: # has previous actions logged\n",
    "                        prev_log = data[s_id]['logs'][len(data[s_id]['logs'])-1] # pick last action\n",
    "                    else:\n",
    "                        prev_log = {} # no available previous action\n",
    "                    log, data[s_id]['tutorial_complete'] = get_logs(s_id, row, prev_log, pca=pca, \n",
    "                                                                    pca_dim=pca_dim, \n",
    "                                                                    tutorial_complete=data[s_id]['tutorial_complete'])\n",
    "                    if log and data[s_id]['tutorial_complete']: # not a movement event\n",
    "                        data[s_id]['logs'].append(log) # add new action (format: dict) to logs\n",
    "                        \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
