{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "#from bias_inspection import detect_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(y_pred, y_true, ids):\n",
    "    data_dict = {}\n",
    "    for i in ids:\n",
    "        if i in data_dict:\n",
    "            continue\n",
    "        data_dict[i] = {'y_pred': [], 'y_true': []}\n",
    "        for y in range(len(y_pred)):\n",
    "            if ids[y] == i:\n",
    "                data_dict[i]['y_pred'].append(y_pred[y])\n",
    "                data_dict[i]['y_true'].append(y_true[y])\n",
    "                \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_scores(data):\n",
    "    pred_scores = []\n",
    "    true_scores = []\n",
    "    mse_scores = []\n",
    "    for s_id in data:\n",
    "        pred_scores.append(mean(data[s_id]['y_pred']))\n",
    "        true_scores.append(mean(data[s_id]['y_true']))\n",
    "        mse_scores.append(mean_squared_error(data[s_id]['y_true'], data[s_id]['y_pred']))\n",
    "        \n",
    "    mean_r2 = r2_score(true_scores, pred_scores)\n",
    "    mean_mse = mean_squared_error(true_scores, pred_scores)\n",
    "    \n",
    "    return {'r2_score': mean_r2, 'mean_mse': mean_mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_scores(y_pred, y_true):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    return {'r2_score': r2, 'mse': mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels):\n",
    "    return [[label1[0] for label1 in labels[0]], [label2[0] for label2 in labels[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test_CV(data, y_pred, y_true, joint_ids, set_name, seq_length=20, \n",
    "                        extension=\"\", inspect_bias=True):\n",
    "    y_pred = format_labels(y_pred)\n",
    "    posttest_dict = create_dict(y_pred[0], y_true[0], joint_ids['ids']) \n",
    "    ratings_dict = create_dict(y_pred[1], y_true[1], joint_ids['response_ids'])\n",
    "    \n",
    "    #if inspect_bias:\n",
    "    #    print(\"ABROCA results for \", set_name)\n",
    "    #    detect_bias(posttest_dict, label_name='posttest') \n",
    "    #    detect_bias(posttest_dict, label_name='ratings')\n",
    "    \n",
    "    result_metrics = {'posttest_macro': get_macro_scores(posttest_dict), \n",
    "                      'rating_macro': get_macro_scores(ratings_dict),\n",
    "                      'posttest_micro': get_micro_scores(y_pred[0], y_true[0]), \n",
    "                      'ratings_micro': get_micro_scores(y_pred[1], y_true[1])}\n",
    "    \n",
    "    return result_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
